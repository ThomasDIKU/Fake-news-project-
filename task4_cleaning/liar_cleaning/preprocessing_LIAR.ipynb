{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Task 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import libraries and load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqJbGIiOnuZ7",
        "outputId": "41452363-2eb1-49ea-8611-daa8bc1901d2"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from cleantext import clean\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "#nltk.download('all')\n",
        "from nltk.probability import FreqDist\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "liar_test_data = pd.read_csv(\"test.tsv\", header = None, sep='\\t', usecols=[1,2], names=['type', 'content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OZvZBQj2oe7X"
      },
      "outputs": [],
      "source": [
        "# Load data as data frame\n",
        "# corpusSample = pd.read_csv(\"250_rows.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean ```content``` variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The function clean_text() does this:\n",
        "#   - all words will be lowercased\n",
        "#   - tabs, new lines and multiple white spaces will be set to single white space\n",
        "#   - numbers, dates, emails, and URLs will be replaced by \"\\<NUM>\", \"\\<DATE>\", \"\\<EMAIL>\" AND \"\\<URL>\", respectively.\n",
        "def clean_text(data):\n",
        "\n",
        "  # Set dates with format DD/MM/YYYY or MM/DD/YYYY to \"<date>\"\n",
        "  data = re.sub(\"[0-9]{1,2}/[0-9]{1,2}/[0-9]{4}\", \"<date>\", data) \n",
        "\n",
        "  # Set dates with the format DD/MM/YY or MM/DD/YY to \"<date>\"\n",
        "  data = re.sub(\"[0-9]{1,2}/[0-9]{1,2}/[0-9]{2}\", \"<date>\", data) \n",
        "\n",
        "  # Set dates with the format DD/MM/YYYY or MM/DD/YYYY to \"<date>\"\n",
        "  data = re.sub(\"[0-9]{1,2}-[0-9]{1,2}-[0-9]{4}\", \"<date>\", data) \n",
        "\n",
        "  # Set dates with the format DD/MM/YY or MM/DD/YY to \"<date>\"\n",
        "  data = re.sub(\"[0-9]{1,2}-[0-9]{1,2}-[0-9]{2}\", \"<date>\", data)\n",
        "\n",
        "  # Consider adding other date formats, like \"Sept 6\", \"September 6, 2019\", etc.\n",
        "\n",
        "  # Use clean() for remaining cleaning\n",
        "  cleaned = clean(data,\n",
        "      fix_unicode=False,           # fix various unicode errors\n",
        "      to_ascii=False,              # transliterate to closest ASCII representation\n",
        "      lower=True,                  # lowercase text\n",
        "      no_line_breaks=True,         # fully strip line breaks as opposed to only normalizing them\n",
        "      no_urls=True,                # replace all URLs with a special token\n",
        "      no_emails=True,              # replace all email addresses with a special token\n",
        "      no_phone_numbers=False,      # replace all phone numbers with a special token\n",
        "      no_numbers=True,             # replace all numbers with a special token\n",
        "      no_digits=False,             # replace all digits with a special token\n",
        "      no_currency_symbols=False,   # replace all currency symbols with a special token\n",
        "      no_punct=False,              # remove punctuations\n",
        "      replace_with_punct=\"\",       # instead of removing punctuations you may replace them\n",
        "      replace_with_url=\"<URL>\",\n",
        "      replace_with_email=\"<EMAIL>\",\n",
        "      replace_with_phone_number=\"<PHONE>\",\n",
        "      replace_with_number=\"<NUM>\",\n",
        "      replace_with_digit=\"0\",\n",
        "      replace_with_currency_symbol=\"<CUR>\",\n",
        "      lang=\"en\"                     # set to 'de' for German special handling\n",
        "  )\n",
        "  return cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "liar_test_data_cleaned = liar_test_data['content'].apply(clean_text) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenize ```content``` variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF7mppLj3h1_",
        "outputId": "162e56b7-4e29-4c4a-c966-0da66acc2596"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.tokenize import MWETokenizer \n",
        "\n",
        "# This is to make sure that \"<num>\", \"<date>\", \"<email>\" and \"<url>\" are \n",
        "# single tokens - and not \"<\", \"num\" and \">\" etc.\n",
        "multiWordsTokenizer = MWETokenizer([('<', 'num', '>'), ('<', 'date', '>'), ('<', 'email', '>'), ('<', 'url', '>')], separator='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function that tokenize a string\n",
        "def tokenize(data_string):\n",
        "\n",
        "    # Word tokenize\n",
        "    data_string = word_tokenize(data_string)\n",
        "\n",
        "    # MAKE '<', 'NUM' and '>' into '<NUM>'. Same for <DATE>, <EMAIL> and <URL>:\n",
        "    data_string = multiWordsTokenizer.tokenize(data_string)\n",
        "\n",
        "    return data_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "liar_test_data_tokenized = liar_test_data_cleaned.apply(tokenize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function that removes stop words from a string\n",
        "def removeStopWords(words):\n",
        "    filteredWords = []\n",
        "\n",
        "    for w in words:\n",
        "        if w not in stop_words:\n",
        "            filteredWords.append(w)\n",
        "    return(filteredWords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove stop words\n",
        "liar_test_data_no_stop_words = liar_test_data_tokenized.apply(removeStopWords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perform stemming on ```content``` variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function that performs stemming on a string\n",
        "def stemming(words):\n",
        "\n",
        "    stemmedWords = []\n",
        "    for w in words:\n",
        "        stemmedWords.append(stemmer.stem(w))\n",
        "    \n",
        "    return(stemmedWords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "liar_test_data_stemmed = liar_test_data_no_stop_words.apply(stemming)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reduction rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocabulary after tokenization: 4293\n",
            "\n",
            "Size of vocabulary after removal of stop words: 4181\n",
            "\n",
            "Size of vocabulary after stemming: 3244\n",
            "\n",
            "Reduction rate of the vocabulary size after removing stopwords: 2.608898206382483\n",
            "\n",
            "Reduction rate of the vocabulary size after stemming: 22.41090648170294\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Using FreqDist() we can see the vocabulary as well as the frequence of each token\n",
        "tokens_after_tokenization = [x.strip(\"'\") for l in liar_test_data_tokenized for x in l]\n",
        "tokens_after_tokenization_vocab = FreqDist(tokens_after_tokenization)\n",
        "\n",
        "tokens_after_removing_stop_words = [x.strip(\"'\") for l in liar_test_data_no_stop_words for x in l]\n",
        "tokens_after_removing_stop_words_vocab = FreqDist(tokens_after_removing_stop_words)\n",
        "\n",
        "tokens_after_stemming = [x.strip(\"'\") for l in liar_test_data_stemmed for x in l]\n",
        "tokens_after_stemming_vocab = FreqDist(tokens_after_stemming)\n",
        "\n",
        "print(f\"Size of vocabulary after tokenization: {len(tokens_after_tokenization_vocab)}\\n\")\n",
        "\n",
        "print(f\"Size of vocabulary after removal of stop words: {len(tokens_after_removing_stop_words_vocab)}\\n\")\n",
        "\n",
        "print(f\"Size of vocabulary after stemming: {len(tokens_after_stemming_vocab)}\\n\")\n",
        "\n",
        "print(f\"Reduction rate of the vocabulary size after removing stopwords: {(len(tokens_after_tokenization_vocab)\n",
        "                                    - len(tokens_after_removing_stop_words_vocab)) / \n",
        "                                    len(tokens_after_tokenization_vocab) * 100}\\n\")\n",
        "\n",
        "print(f\"Reduction rate of the vocabulary size after stemming: {(len(tokens_after_removing_stop_words_vocab)\n",
        "                                    - len(tokens_after_stemming_vocab)) / \n",
        "                                    len(tokens_after_removing_stop_words_vocab) * 100}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add preprocessed 'corpus' variable to corpus sample:\n",
        "liar_preprocessed = liar_test_data\n",
        "liar_preprocessed['content'] = liar_test_data_stemmed\n",
        "liar_preprocessed.to_csv(\"liar_test_data_preprocessed.csv\", index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
