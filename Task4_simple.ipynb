{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find top 10000 vocab + Logistic Rgression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from scipy import sparse\n",
    "import json\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Evaluation on simple logistic regression model, on fake news corpus test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the logistic regression model \n",
    "logistic_model = joblib.load('logistic_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data test set \n",
    "X_test_scaled = sparse.load_npz(\"X_test_scaled.npz\")\n",
    "y_test = pd.read_csv(\"y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with test\n",
    "y_pred = logistic_model.predict(X_test_scaled)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Total F1-score:\", f1)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: 2, evaluate the logistic model on liar data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load \"LIAR\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed LIAR-dataset\n",
    "liar_data = pd.read_csv(\"liar_dataset/liar_full_dataset_preprocessed.csv\", usecols=['type', 'content'], dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the liar dataset, the 'content' variable has been cleaned and tokenized - and stop words has \n",
    "been removed and all tokens has been stemmed. This has been done with the use of our preprocessing\n",
    "pipeline. We still need to remove rows where either 'type' or 'content' is NaN, though. And we need to \n",
    "remove duplicated rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove data points where either 'type' or 'content' is NaN\n",
    "liar_data = liar_data[liar_data['type'].notna() & liar_data['content'].notna()]\n",
    "\n",
    "# Remove duplicates\n",
    "liar_data = liar_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_mapping_liar = {\n",
    "    'true': 'reliable', \n",
    "    'false': 'fake', \n",
    "    'half-true': 'fake', \n",
    "    'pants-fire': 'fake', \n",
    "    'barely-true': 'reliable',\n",
    "    'mostly-true': 'reliable'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_data[\"label\"] = liar_data[\"type\"].map(type_mapping_liar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the top_10000_words\n",
    "with open(\"top_10000_words.json\", \"r\", encoding=\"utf-8\") as fil:\n",
    "    top_10000_words = json.load(fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(vocabulary=top_10000_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform liar data\n",
    "X_liar = vectorizer.transform(liar_data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scaleren\n",
    "scaler = joblib.load('scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "X_liar_scaled = scaler.transform(X_liar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_liar = liar_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_liar = logistic_model.predict(X_liar_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation on liar data \n",
    "f1_liar = f1_score(y_liar, y_pred_liar, average='weighted')\n",
    "print(\"Total F1-score:\", f1_liar)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_liar, y_pred_liar))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_liar, y_pred_liar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ---- 3. Convert text to Bag-of-Words feature matrix ----\n",
    "\n",
    "content_as_lists = data['content'].apply(ast.literal_eval)\n",
    "\n",
    "all_words = content_as_lists.explode().tolist()\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "top_10000_words = [word for word, _ in word_counts.most_common(10000)]\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary=top_10000_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform training, valid og test data\n",
    "X_train = vectorizer.transform(train['content'])\n",
    "X_valid = vectorizer.transform(valid['content'])\n",
    "X_test = vectorizer.transform(test['content'])\n",
    "\n",
    "# Transform LIAR test data to Bag-of-Words\n",
    "# X_test_liar = vectorizer.transform(liar_test_data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MaxAbsScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# LIAR not scaled\n",
    "\n",
    "# Labels\n",
    "y_train = train['label']\n",
    "y_valid = valid['label']\n",
    "y_test = test['label']\n",
    "\n",
    "y_test_liar = liar_test_data['label']\n",
    "\n",
    "# ---- 4. Train Logistic Regression model ----\n",
    "\n",
    "clf = LogisticRegression(max_iter=100000, solver='saga', random_state=42)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# ---- 5. Evaluate model ----\n",
    "\n",
    "y_pred_liar = clf.predict(X_test_liar)\n",
    "\n",
    "# ---- 6. Evaluate model on LIAR test. Notice that it isn't scaled since scaling gave an even worse result ----\n",
    "\n",
    "f1 = f1_score(y_test_liar, y_pred_liar, average='weighted')  # eller 'macro'\n",
    "print(\"Total F1-score: LIAR dataset:\", f1)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_liar, y_pred_liar))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_liar, y_pred_liar))\n",
    "\n",
    "\n",
    "\n",
    "# ---- Original test with test set (remove # for comparison)----\n",
    "# print(\"Original test with test set - classification report\")\n",
    "# print(classification_report(y_test, clf.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
