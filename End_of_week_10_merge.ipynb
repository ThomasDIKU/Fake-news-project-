{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find top 10000 vocab + Logistic Rgression\n",
    "\n",
    "End of week 10: tasks 0-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import requests\n",
    "import ast\n",
    "\n",
    "# Download nltk packages if necessary\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# ---- 1. Load and prepare data ----\n",
    "\n",
    "# Mapping of types to \"fake\" or \"reliable\"\n",
    "type_mapping = {\n",
    "    'unreliable': 'fake',\n",
    "    'fake': 'fake',\n",
    "    'conspiracy': 'fake',\n",
    "    'bias': 'fake',\n",
    "    'junksci': 'fake',\n",
    "    'clickbait': 'reliable',\n",
    "    'reliable' : 'reliable',\n",
    "    'state': 'fake',\n",
    "    'political': 'reliable',\n",
    "    'satire': 'fake',\n",
    "    'hate': 'fake',\n",
    "    'rumor': 'fake',\n",
    "}\n",
    "\n",
    "# Load data and filter relevant columns\n",
    "data = pd.read_csv(\"15,000_rows_preprocessed.csv\", usecols=[\"content\", \"type\"], dtype=str)\n",
    "#data = pd.read_csv(\"995,000_rows_preprocessed.csv\", usecols=[\"content\", \"type\"], dtype=str)\n",
    "\n",
    "# ---- Load BBC-data and add to dataset ----\n",
    "bbc_data = pd.read_csv(\"BBC_preprocessed.csv\", usecols=[\"content\", \"type\"], dtype=str)\n",
    "data = pd.concat([data, bbc_data], ignore_index=True)\n",
    "\n",
    "# Remove rows with unknown type\n",
    "data = data[data['type'] != 'unknown']\n",
    "\n",
    "# Map types to labels\n",
    "data[\"label\"] = data[\"type\"].map(type_mapping)\n",
    "\n",
    "# Remove NaN\n",
    "data = data.dropna(subset=[\"label\"])\n",
    "\n",
    "# ---- 2. Split dataset to training, validation and test (80/10/10) ----\n",
    "\n",
    "train, valid, test = np.split(\n",
    "    data.sample(frac=1, random_state=42),  # Shuffle\n",
    "    [int(0.8 * len(data)), int(0.9 * len(data))]  # Index for split\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ---- 3. Convert text to Bag-of-Words feature matrix ----\n",
    "\n",
    "content_as_lists = data['content'].apply(ast.literal_eval)\n",
    "\n",
    "all_words = content_as_lists.explode().tolist()\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "top_10000_words = [word for word, _ in word_counts.most_common(10000)]\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary=top_10000_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform training, valid og test data\n",
    "X_train = vectorizer.transform(train['content'])\n",
    "X_valid = vectorizer.transform(valid['content'])\n",
    "X_test = vectorizer.transform(test['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.77      0.83      0.80       676\n",
      "    reliable       0.82      0.75      0.79       694\n",
      "\n",
      "    accuracy                           0.79      1370\n",
      "   macro avg       0.79      0.79      0.79      1370\n",
      "weighted avg       0.79      0.79      0.79      1370\n",
      "\n",
      "[[562 114]\n",
      " [171 523]]\n"
     ]
    }
   ],
   "source": [
    "# Convert to dense matrix (since StandardScaler does not support sparse matrices directly)\n",
    "# X_train_dense = X_train.toarray()\n",
    "# X_valid_dense = X_valid.toarray()\n",
    "# X_test_dense = X_test.toarray()\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Labels\n",
    "y_train = train['label']\n",
    "y_valid = valid['label']\n",
    "y_test = test['label']\n",
    "\n",
    "# ---- 4. Train Logistic Regression model ----\n",
    "\n",
    "clf = LogisticRegression(max_iter=100000, solver='saga', random_state=42)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# ---- 5. Evaluate model ----\n",
    "\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# ---- 6. Validate model ---- (remove hashtags)\n",
    "\n",
    "# y_pred = clf.predict(X_valid_scaled)\n",
    "\n",
    "# print(classification_report(y_valid, y_pred))\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# print(confusion_matrix(y_valid, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---- 5. Indlæs top 10.000 ord fra CSV og konverter tekst til Bag-of-Words feature matrix ----\n",
    "\n",
    "# # Indlæs ordforrådet fra CSV\n",
    "# df_vocab = pd.read_csv(\"top_10000_vocabulary.csv\")\n",
    "# top_10000_words = df_vocab[\"Word\"].tolist()  # Konverter til liste\n",
    "\n",
    "# # Opret CountVectorizer med det indlæste ordforråd\n",
    "# vectorizer = CountVectorizer(vocabulary=top_10000_words) # BoW bag of words\n",
    "\n",
    "# # Transformer træning, valid og test data\n",
    "# X_train = vectorizer.transform(train['content'])\n",
    "# X_valid = vectorizer.transform(valid['content'])\n",
    "# X_test = vectorizer.transform(test['content'])\n",
    "\n",
    "# # Labels\n",
    "# y_train = train['label']\n",
    "# y_valid = valid['label']\n",
    "# y_test = test['label']\n",
    "\n",
    "# # ---- 6. Træn Logistic Regression model ----\n",
    "\n",
    "# clf = LogisticRegression(max_iter=5000, solver='saga', random_state=42)\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# # ---- 7. Evaluer modellen ----\n",
    "\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# # Udskriv resultater\n",
    "# print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kode kopieret fra YouTube: https://www.youtube.com/watch?v=aL21Y-u0SRs&t=212s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# model = LogisticRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "# y_pred = model.predict(X_test)\n",
    "# print(model.score(X_test, y_test))\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gammel version 1... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import nltk\n",
    "# from collections import Counter\n",
    "# from nltk.stem import PorterStemmer\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# # Ensure NLTK resources are available\n",
    "# # nltk.download('punkt')\n",
    "# # nltk.download('stopwords')\n",
    "\n",
    "# # Load data efficiently\n",
    "# data = pd.read_csv(\"995,000_rows.csv\", usecols=[\"content\"], dtype=str)\n",
    "\n",
    "# # Shuffle and split dataset (80% train, 10% validation, 10% test)\n",
    "# train, valid, test = np.split(\n",
    "#     data.sample(frac=1, random_state=42),  # Shuffle dataset\n",
    "#     [int(0.8 * len(data)), int(0.9 * len(data))]  # Indices for 80% train, 10% valid, 10% test\n",
    "# )\n",
    "\n",
    "# # Precompile regex patterns\n",
    "# date_regex = re.compile(r\"\"\"\\b(\n",
    "#     (?:\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}) |\n",
    "#     (?:\\d{4}[./-]\\d{1,2}[./-]\\d{1,2}) |\n",
    "#     (?:\\b\\d{1,2}(st|nd|rd|th)?\\s+(of\\s+)?[A-Za-z]+\\s+\\d{4}\\b) |\n",
    "#     (?:\\b[A-Za-z]+\\s+\\d{1,2}(st|nd|rd|th)?,?\\s+\\d{4}\\b) |\n",
    "#     (?:\\b[A-Za-z]+\\s+\\d{4}\\b)\n",
    "# )\\b\"\"\", re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "# number_regex = re.compile(r'\\d+')\n",
    "# email_regex = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b')\n",
    "# url_regex = re.compile(r'\\b(?:http[s]?://|www\\.)[^\\s<>\"]+\\b')\n",
    "# whitespace_regex = re.compile(r'\\s+')\n",
    "\n",
    "# # Function to clean text\n",
    "# def clean_text(text):\n",
    "#     text = text.lower()\n",
    "#     text = whitespace_regex.sub(' ', text)\n",
    "#     text = date_regex.sub(\"<DATE>\", text)\n",
    "#     text = url_regex.sub(\"<URL>\", text)\n",
    "#     text = email_regex.sub(\"<EMAIL>\", text)\n",
    "#     text = number_regex.sub(\"<NUM>\", text)\n",
    "#     return text\n",
    "\n",
    "# # Apply text cleaning to each split\n",
    "# train['content'] = train['content'].fillna('').map(clean_text)\n",
    "# valid['content'] = valid['content'].fillna('').map(clean_text)\n",
    "# test['content'] = test['content'].fillna('').map(clean_text)\n",
    "\n",
    "# # Tokenization and stopword filtering (only on train set)\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# tokens = [\n",
    "#     word for sentence in train['content'] \n",
    "#     for word in word_tokenize(sentence) \n",
    "#     if word.isalpha() and word not in stop_words\n",
    "# ]\n",
    "\n",
    "# # Stemming\n",
    "# stemmer = PorterStemmer()\n",
    "# stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# # Compute word frequencies\n",
    "# word_counts = Counter(stemmed_tokens)\n",
    "\n",
    "# # Extract top 10,000 words\n",
    "# top_10000 = word_counts.most_common(10000)\n",
    "\n",
    "# # Export to CSV\n",
    "# df_top_words = pd.DataFrame(top_10000, columns=['Word', 'Frequency'])\n",
    "# df_top_words.to_csv(\"top_10000_vocabulary.csv\", index=False)\n",
    "\n",
    "# print(\"Top 10,000 words saved to top_10000_vocabulary.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
