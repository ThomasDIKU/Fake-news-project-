{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Task 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import libraries and load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqJbGIiOnuZ7",
        "outputId": "41452363-2eb1-49ea-8611-daa8bc1901d2"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from cleantext import clean\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "#nltk.download('all')\n",
        "from nltk.probability import FreqDist\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZvZBQj2oe7X"
      },
      "outputs": [],
      "source": [
        "# Load data as data frame\n",
        "corpusSample = pd.read_csv(\"250_rows.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean ```content``` variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The function clean_text() does this:\n",
        "#   - all words will be lowercased\n",
        "#   - tabs, new lines and multiple white spaces will be set to single white space\n",
        "#   - numbers, dates, emails, and URLs will be replaced by \"\\<NUM>\", \"\\<DATE>\", \"\\<EMAIL>\" AND \"\\<URL>\", respectively.\n",
        "def clean_text(data):\n",
        "\n",
        "  # Set dates with format DD/MM/YYYY or MM/DD/YYYY to \"<date>\"\n",
        "  data = re.sub(\"[0-9]{1,2}/[0-9]{1,2}/[0-9]{4}\", \"<date>\", data) \n",
        "\n",
        "  # Set dates with the format DD/MM/YY or MM/DD/YY to \"<date>\"\n",
        "  data = re.sub(\"[0-9]{1,2}/[0-9]{1,2}/[0-9]{2}\", \"<date>\", data) \n",
        "\n",
        "  # Set dates with the format DD/MM/YYYY or MM/DD/YYYY to \"<date>\"\n",
        "  data = re.sub(\"[0-9]{1,2}-[0-9]{1,2}-[0-9]{4}\", \"<date>\", data) \n",
        "\n",
        "  # Set dates with the format DD/MM/YY or MM/DD/YY to \"<date>\"\n",
        "  data = re.sub(\"[0-9]{1,2}-[0-9]{1,2}-[0-9]{2}\", \"<date>\", data)\n",
        "\n",
        "  # Consider adding other date formats, like \"Sept 6\", \"September 6, 2019\", etc.\n",
        "\n",
        "  # Use clean() for remaining cleaning\n",
        "  cleaned = clean(data,\n",
        "      fix_unicode=False,           # fix various unicode errors\n",
        "      to_ascii=False,              # transliterate to closest ASCII representation\n",
        "      lower=True,                  # lowercase text\n",
        "      no_line_breaks=True,         # fully strip line breaks as opposed to only normalizing them\n",
        "      no_urls=True,                # replace all URLs with a special token\n",
        "      no_emails=True,              # replace all email addresses with a special token\n",
        "      no_phone_numbers=False,      # replace all phone numbers with a special token\n",
        "      no_numbers=True,             # replace all numbers with a special token\n",
        "      no_digits=False,             # replace all digits with a special token\n",
        "      no_currency_symbols=False,   # replace all currency symbols with a special token\n",
        "      no_punct=False,              # remove punctuations\n",
        "      replace_with_punct=\"\",       # instead of removing punctuations you may replace them\n",
        "      replace_with_url=\"<URL>\",\n",
        "      replace_with_email=\"<EMAIL>\",\n",
        "      replace_with_phone_number=\"<PHONE>\",\n",
        "      replace_with_number=\"<NUM>\",\n",
        "      replace_with_digit=\"0\",\n",
        "      replace_with_currency_symbol=\"<CUR>\",\n",
        "      lang=\"en\"                     # set to 'de' for German special handling\n",
        "  )\n",
        "  return cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "content_sample_cleaned = corpusSample['content'].apply(clean_text) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenize ```content``` variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF7mppLj3h1_",
        "outputId": "162e56b7-4e29-4c4a-c966-0da66acc2596"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.tokenize import MWETokenizer \n",
        "\n",
        "# This is to make sure that \"<num>\", \"<date>\", \"<email>\" and \"<url>\" are \n",
        "# single tokens - and not \"<\", \"num\" and \">\" etc.\n",
        "multiWordsTokenizer = MWETokenizer([('<', 'num', '>'), ('<', 'date', '>'), ('<', 'email', '>'), ('<', 'url', '>')], separator='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function that tokenize a string\n",
        "def tokenize(data_string):\n",
        "\n",
        "    # Word tokenize\n",
        "    data_string = word_tokenize(data_string)\n",
        "\n",
        "    # MAKE '<', 'NUM' and '>' into '<NUM>'. Same for <DATE>, <EMAIL> and <URL>:\n",
        "    data_string = multiWordsTokenizer.tokenize(data_string)\n",
        "\n",
        "    return data_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "content_sample_tokenized = content_sample_cleaned.apply(tokenize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function that removes stop words from a string\n",
        "def removeStopWords(words):\n",
        "    filteredWords = []\n",
        "\n",
        "    for w in words:\n",
        "        if w not in stop_words:\n",
        "            filteredWords.append(w)\n",
        "    return(filteredWords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove stop words\n",
        "content_sample_no_stop_words = content_sample_tokenized.apply(removeStopWords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perform stemming on ```content``` variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function that performs stemming on a string\n",
        "def stemming(words):\n",
        "\n",
        "    stemmedWords = []\n",
        "    for w in words:\n",
        "        stemmedWords.append(stemmer.stem(w))\n",
        "    \n",
        "    return(stemmedWords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "content_sample_stemmed = content_sample_no_stop_words.apply(stemming)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reduction rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using FreqDist() we can see the vocabulary as well as the frequence of each token\n",
        "tokens_after_tokenization = [x.strip(\"'\") for l in content_sample_tokenized for x in l]\n",
        "tokens_after_tokenization_vocab = FreqDist(tokens_after_tokenization)\n",
        "\n",
        "tokens_after_removing_stop_words = [x.strip(\"'\") for l in content_sample_no_stop_words for x in l]\n",
        "tokens_after_removing_stop_words_vocab = FreqDist(tokens_after_removing_stop_words)\n",
        "\n",
        "tokens_after_stemming = [x.strip(\"'\") for l in content_sample_stemmed for x in l]\n",
        "tokens_after_stemming_vocab = FreqDist(tokens_after_stemming)\n",
        "\n",
        "print(f\"Size of vocabulary after tokenization: {len(tokens_after_tokenization_vocab)}\\n\")\n",
        "\n",
        "print(f\"Size of vocabulary after removal of stop words: {len(tokens_after_removing_stop_words_vocab)}\\n\")\n",
        "\n",
        "print(f\"Size of vocabulary after stemming: {len(tokens_after_stemming_vocab)}\\n\")\n",
        "\n",
        "print(f\"Reduction rate of the vocabulary size after removing stopwords: {(len(tokens_after_tokenization_vocab)\n",
        "                                    - len(tokens_after_removing_stop_words_vocab)) / \n",
        "                                    len(tokens_after_tokenization_vocab) * 100}\\n\")\n",
        "\n",
        "print(f\"Reduction rate of the vocabulary size after stemming: {(len(tokens_after_removing_stop_words_vocab)\n",
        "                                    - len(tokens_after_stemming_vocab)) / \n",
        "                                    len(tokens_after_removing_stop_words_vocab) * 100}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add preprocessed 'corpus' variable to corpus sample:\n",
        "corpusSamplePreprocessed = corpusSample\n",
        "corpusSamplePreprocessed['content'] = content_sample_stemmed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Task 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initial clean up: Remove non-relevant features and rows with invalid values - and remove row duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data as data frame. Load either full corpus or sample with 10,000 rows.\n",
        "\n",
        "#corpus = pd.read_csv(\"995,000_rows.csv\")\n",
        "corpus = pd.read_csv(\"15,000_rows.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove non-relevant features\n",
        "corpus = corpus[['domain','type', 'content', 'title', 'authors', 'meta_description']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove rows with invalid values\n",
        "corpus = corpus.drop(corpus[corpus['type'] == '2018-02-10 13:43:39.521661'].index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove data points where either 'type' or 'content' is NaN\n",
        "corpus = corpus[corpus['type'].notna() & corpus['content'].notna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove duplicates - there are 66.232 duplicated rows in full dataset.\n",
        "corpus = corpus.drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing: Clean, tokenize, remove stop words and perform stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean 'content' and save data frame as .csv file\n",
        "corpus['content'] = corpus['content'].apply(clean_text)\n",
        "corpus.to_csv('corpus_cleaned.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize 'content' and save data frame as .csv file\n",
        "corpus['content'] = corpus['content'].apply(tokenize)\n",
        "corpus.to_csv('corpus_tokenized.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove stop words from 'content' and save data frame as .csv file\n",
        "corpus['content'] = corpus['content'].apply(removeStopWords)\n",
        "corpus.to_csv('corpus_no_stop_words.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform stemming on 'content' and save data frame as .csv file\n",
        "corpus['content'] = corpus['content'].apply(stemming)\n",
        "corpus.to_csv('995,000_rows_preprocessed.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 3 - the three questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Data frames has labeled axes, as opposed to for instance numpy arrays. And it is possible to get a nice spreadsheet representation of the data set.\n",
        "1. \n",
        "    - ```Authors``` variable has 44% missing values and ```meta_description``` has 53% -  making it hard to use them in a model.\n",
        "    - The type value ```2018-02-10 13:43:39.521661``` only has one news article and it looks like the article has been mislabeled (the name is weird and all other domains has at least 8779 articles). Should be removed.\n",
        "    - 'type' has 47786 missing values and 'content' has 12 missing values. The data points (rows) where either of these two values are missing should be removed.\n",
        "1. Include for instance: Number of features and data points. Number of missing values for each feature. Number of distinct values for relevant features (and what the categorical values are). Data type of each feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 3 - non-trivial observation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section we will see that...\n",
        "\n",
        "\n",
        "- All texts from a particular domain is of the same type,\n",
        "\n",
        "- Few domains accounts for a majority of the total number of articles.\n",
        "\n",
        "\n",
        "- The distribution of article types are very uneven. For instance: There are 25 times as many articles of type 'reliable' than type 'hate',\n",
        "\n",
        "- The average length of articles (the token count) for each article type varies greatly. 'hate' articles has the highest average token count and 'satire' has the lowest.\n",
        "\n",
        "\n",
        "- There is a large variation in number of tokens in 'content' in each article. For instance: The longest article has 21730 tokens, and the shortest has just two tokens. However: only very few articles has a very high amount of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load preprocessed corpus (if you want to skip the preprocessing steps above).\n",
        "# Load either full preprocessed dataset or sample with 15,000 rows\n",
        "\n",
        "#corpus = pd.read_csv(\"15,000_rows_preprocessed.csv\")\n",
        "#corpus = pd.read_csv(\"995,000_rows_preprocessed.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# After loading the .csv file as dataframe remember to convert the data type of \n",
        "# 'content' from string back to list:\n",
        "\n",
        "corpus['content'] = corpus['content'].apply(ast.literal_eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Looking into 'domain'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Number of unique values in 'domain': {len(set(corpus['domain']))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here we see that all texts from a particular domain is of the same type\n",
        "boolian_value = True\n",
        "for x in set(corpus['domain']):\n",
        "    df_subset = corpus[corpus[\"domain\"] == x]\n",
        "    if(len(set(df_subset['type']))) != 1:\n",
        "        print(x)\n",
        "        boolian_value = False\n",
        "\n",
        "if boolian_value:\n",
        "    print(\"All texts from a particular domain is of the same type!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here is the number of news texts for each domain\n",
        "counts_domain = corpus['domain'].value_counts()\n",
        "counts_domain_df = counts_domain.rename_axis('unique_values').reset_index(name='counts')\n",
        "\n",
        "# The domains with the most articles\n",
        "print(f\"The 10 domains with the most articles:\\n{counts_domain_df[0:10]}\\n\")\n",
        "\n",
        "# The domains with the fewest articles\n",
        "print(f\"The 10 domains with the fewest articles:\\n{counts_domain_df[-10:]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot of the distribution of articles across the different domains\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.xlabel('Domain in decending order; 1 is the domain with most articles, etc.')\n",
        "plt.ylabel('Number of articles')\n",
        "plt.title('Distribution of articles across the different domains')\n",
        "plt.xticks([1,100,200, 300, 400, 500, 600])\n",
        "plt.plot(counts_domain_df.index, counts_domain_df['counts'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Looking into 'type'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Number of unique values in 'type': {len(set(corpus['type']))}\\n\")\n",
        "\n",
        "# Here is the number of news texts for each domain\n",
        "counts_type = corpus['type'].value_counts()\n",
        "counts_type_df = counts_type.rename_axis('unique_values').reset_index(name='counts')\n",
        "\n",
        "print(f\"Distribution of articles type:\\n{counts_type_df}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot of the distribution of articles across the different types\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.title('Distribution of article types')\n",
        "plt.xlabel('Type')\n",
        "plt.ylabel('Number of articles')\n",
        "plt.bar(counts_type_df['unique_values'], counts_type_df['counts'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Looking into 'content'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate lengths of 'content' strings\n",
        "corpus['content_length'] = corpus.content.str.len()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "content_length = corpus['content_length'].sort_values(ascending=False)\n",
        "print(f\"The articles with most tokens has {content_length.iloc[0]} tokens.\")\n",
        "print(f\"The articles with fewest tokens has {content_length.iloc[-1]} tokens.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Only 5,000 articles (or {round(5000/corpus.shape[0] * 100, 2)}% of the articles) has more than {content_length.iloc[5000]} tokens.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Average tokens per article is {round(content_length.mean(), 2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot of the number tokens in each article\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.title('Number of tokens in each article')\n",
        "plt.xlabel('Articles in order: Article nr. 0 has highest number of tokens, etc.')\n",
        "plt.ylabel('Number of tokens')\n",
        "plt.xticks([0,10000, 200000,400000, 600000, 800000])\n",
        "plt.xticks(rotation=70)\n",
        "plt.plot(corpus.index, corpus['content_length'].sort_values(ascending=False))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find average length of articles (token count) for each article type.\n",
        "types = set(corpus['type'])\n",
        "types_df = pd.DataFrame(types)\n",
        "types_df.columns = ['type']\n",
        "types_df[\"average_token_count\"] = np.nan\n",
        "\n",
        "\n",
        "for index, row in types_df.iterrows():\n",
        "    df_temp = corpus.loc[corpus['type'] == row['type']]\n",
        "    types_df.at[index, 'average_token_count'] = sum(df_temp['content_length'])/df_temp.shape[0]\n",
        "\n",
        "types_df = types_df.sort_values(\"average_token_count\", ascending=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the average length of articles (the token count) for each article type.\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.title('Distribution of article types')\n",
        "plt.xlabel('Type')\n",
        "plt.ylabel('Average length of articles')\n",
        "plt.bar(types_df['type'], types_df['average_token_count'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train, valid, test = np.split(corpus.sample(frac=1, random_state=42), [int(0.8*len(corpus)), int(0.9*len(corpus))])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
