{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - evaluate logistic regression model on FakeNewsCorpus test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from scipy import sparse\n",
    "import json\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thom0c49\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.6.1 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Load the logistic regression model \n",
    "logistic_model = joblib.load('misc/logistic_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data test set \n",
    "X_test_scaled = sparse.load_npz(\"misc/X_test_scaled.npz\")\n",
    "y_test = pd.read_csv(\"misc/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total F1-score: 0.8093849082493505\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.90      0.82      0.86     35942\n",
      "    reliable       0.64      0.77      0.70     15085\n",
      "\n",
      "    accuracy                           0.80     51027\n",
      "   macro avg       0.77      0.80      0.78     51027\n",
      "weighted avg       0.82      0.80      0.81     51027\n",
      "\n",
      "Confusion Matrix:\n",
      "[[29411  6531]\n",
      " [ 3435 11650]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with test\n",
    "y_pred = logistic_model.predict(X_test_scaled)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Total F1-score:\", f1)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - evaluate the logistic regression model on the LIAR data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed LIAR-dataset\n",
    "liar_data = pd.read_csv(\"liar_data/liar_full_dataset_preprocessed.csv\", usecols=['type', 'content'], dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the liar dataset, the 'content' variable has been cleaned and tokenized - and stop words has \n",
    "been removed and all tokens has been stemmed. This has been done with the use of our preprocessing\n",
    "pipeline. We still need to remove rows where either 'type' or 'content' is NaN, though. And we need to \n",
    "remove duplicated rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove data points where either 'type' or 'content' is NaN\n",
    "liar_data = liar_data[liar_data['type'].notna() & liar_data['content'].notna()]\n",
    "\n",
    "# Remove duplicates\n",
    "liar_data = liar_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map types to 'fake' or 'reliable'\n",
    "type_mapping_liar = {\n",
    "    'true': 'reliable', \n",
    "    'false': 'fake', \n",
    "    'half-true': 'fake', \n",
    "    'pants-fire': 'fake', \n",
    "    'barely-true': 'reliable',\n",
    "    'mostly-true': 'reliable'\n",
    "}\n",
    "\n",
    "liar_data[\"label\"] = liar_data[\"type\"].map(type_mapping_liar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the top_10000_words (top 10000 in FakeNewsCorpus train data)\n",
    "with open(\"misc/top_10000_words.json\", \"r\", encoding=\"utf-8\") as fil:\n",
    "    top_10000_words = json.load(fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'content' to Bag-of-Words feature matrix\n",
    "vectorizer = CountVectorizer(vocabulary=top_10000_words)\n",
    "X_liar = vectorizer.transform(liar_data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the scaler (made in Part 2 on the FakeNewsCorpus training data)\n",
    "scaler = joblib.load('misc/scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "X_liar_scaled = scaler.transform(X_liar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label object\n",
    "y_liar = liar_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction\n",
    "y_pred_liar = logistic_model.predict(X_liar_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total F1-score: 0.4646581043175861\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.48      0.74      0.59      4936\n",
      "    reliable       0.52      0.27      0.35      5290\n",
      "\n",
      "    accuracy                           0.49     10226\n",
      "   macro avg       0.50      0.50      0.47     10226\n",
      "weighted avg       0.50      0.49      0.46     10226\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3653 1283]\n",
      " [3887 1403]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on liar data \n",
    "f1_liar = f1_score(y_liar, y_pred_liar, average='weighted')\n",
    "print(\"Total F1-score:\", f1_liar)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_liar, y_pred_liar))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_liar, y_pred_liar))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
