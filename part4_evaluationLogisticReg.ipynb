{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - evaluate logistic regression model on FakeNewsCorpus test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from scipy import sparse\n",
    "import json\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the logistic regression model \n",
    "logistic_model = joblib.load('logistic_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data test set \n",
    "X_test_scaled = sparse.load_npz(\"X_test_scaled.npz\")\n",
    "y_test = pd.read_csv(\"y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with test\n",
    "y_pred = logistic_model.predict(X_test_scaled)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Total F1-score:\", f1)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - evaluate the logistic regression model on the LIAR data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed LIAR-dataset\n",
    "liar_data = pd.read_csv(\"liar_dataset/liar_full_dataset_preprocessed.csv\", usecols=['type', 'content'], dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the liar dataset, the 'content' variable has been cleaned and tokenized - and stop words has \n",
    "been removed and all tokens has been stemmed. This has been done with the use of our preprocessing\n",
    "pipeline. We still need to remove rows where either 'type' or 'content' is NaN, though. And we need to \n",
    "remove duplicated rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove data points where either 'type' or 'content' is NaN\n",
    "liar_data = liar_data[liar_data['type'].notna() & liar_data['content'].notna()]\n",
    "\n",
    "# Remove duplicates\n",
    "liar_data = liar_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map types to 'fake' or 'reliable'\n",
    "type_mapping_liar = {\n",
    "    'true': 'reliable', \n",
    "    'false': 'fake', \n",
    "    'half-true': 'fake', \n",
    "    'pants-fire': 'fake', \n",
    "    'barely-true': 'reliable',\n",
    "    'mostly-true': 'reliable'\n",
    "}\n",
    "\n",
    "liar_data[\"label\"] = liar_data[\"type\"].map(type_mapping_liar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the top_10000_words (top 10000 in FakeNewsCorpus train data)\n",
    "with open(\"top_10000_words.json\", \"r\", encoding=\"utf-8\") as fil:\n",
    "    top_10000_words = json.load(fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'content' to Bag-of-Words feature matrix\n",
    "vectorizer = CountVectorizer(vocabulary=top_10000_words)\n",
    "X_liar = vectorizer.transform(liar_data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the scaler (made in Part 2 on the FakeNewsCorpus training data)\n",
    "scaler = joblib.load('scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "X_liar_scaled = scaler.transform(X_liar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label object\n",
    "y_liar = liar_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction\n",
    "y_pred_liar = logistic_model.predict(X_liar_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on liar data \n",
    "f1_liar = f1_score(y_liar, y_pred_liar, average='weighted')\n",
    "print(\"Total F1-score:\", f1_liar)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_liar, y_pred_liar))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_liar, y_pred_liar))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
